{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNmHcyLUwRwnSEM4acbGfyu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vizcayal/aha_moment/blob/main/grpo_training_qwen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import Libraries"
      ],
      "metadata": {
        "id": "AZq2iJfzloPs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59HjNxijjm3x"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import logging\n",
        "modules = list(sys.modules.keys())\n",
        "import torch\n",
        "import re\n",
        "\n",
        "#installing unsloth to fine-tune LLM models with Reinforcement Learning and Low-Rank Adapataion (LoRA)\n",
        "!pip install unsloth\n",
        "#installing vllm for fast inference, and reducing latency\n",
        "!pip install vllm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel, PatchFastRL\n",
        "from unsloth import is_bfloat16_supported             #check if bfloat16 is supported\n",
        "from datasets import load_dataset, Dataset\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "from vllm import SamplingParams"
      ],
      "metadata": {
        "id": "KR8TbnFwlnfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PatchFastRL(\"GRPO\", FastLanguageModel)                # apply patch for training LLMs with Group Relative Policy Optimization (GRPO) used in deepseek\n",
        "max_seq_length = 512                                  # max sequencen length\n",
        "lora_rank = 32                                        # dim for lora matrix\n",
        "\n",
        "#load the Llama 3.1-8B model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "                                                    model_name = \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
        "                                                    max_seq_length = max_seq_length,\n",
        "                                                    load_in_4bit = True,                                    # False for LoRA 16bit\n",
        "                                                    fast_inference = True,                                  # Enable vLLM fast inference\n",
        "                                                    max_lora_rank = lora_rank,\n",
        "                                                    gpu_memory_utilization = 0.6,\n",
        "                                                    )\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(                                                                   #Parameter efficient Fine-Tuning (peft)\n",
        "                                        model,\n",
        "                                        r = lora_rank,\n",
        "                                        target_modules = [\n",
        "                                                          \"q_proj\",\n",
        "                                                          \"k_proj\",\n",
        "                                                          \"v_proj\",\n",
        "                                                          \"o_proj\",\n",
        "                                                          \"gate_proj\",\n",
        "                                                          \"up_proj\",\n",
        "                                                          \"down_proj\",\n",
        "                                                        ],\n",
        "                                        lora_alpha = lora_rank,\n",
        "                                        use_gradient_checkpointing = \"unsloth\",\n",
        "                                        random_state = 3407,\n",
        "                                        )"
      ],
      "metadata": {
        "id": "-PhXzsHzRRvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
        "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
        "    data = data.map(lambda x: { # type: ignore\n",
        "        'prompt': [\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': x['question']}\n",
        "        ],\n",
        "        'answer': extract_hash_answer(x['answer'])\n",
        "    }) # type: ignore\n",
        "    return data # type: ignore\n",
        "\n",
        "# Reward Functions\n",
        "\n",
        "\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    q = prompts[0][-1]['content']\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
        "\n",
        "# reward in case of digit\n",
        "def int_reward_func(completions, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
        "\n",
        "# reward in case of following strictly the format\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "# reward in case of following strictly the format\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def count_xml(text) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]\n"
      ],
      "metadata": {
        "id": "8bli1HkWpZcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up Parameters for GRPO training"
      ],
      "metadata": {
        "id": "DgK1TD-oCijP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = GRPOConfig(\n",
        "                          use_vllm = True,\n",
        "                          learning_rate = 5e-6,\n",
        "                          adam_beta1 = 0.9,\n",
        "                          adam_beta2 = 0.99,\n",
        "                          weight_decay = 0.1,\n",
        "                          warmup_ratio = 0.1,\n",
        "                          lr_scheduler_type = 'cosine',\n",
        "                          optim = 'paged_adamw_8bit',\n",
        "                          logging_steps = 1,\n",
        "                          bf16 = is_bfloat16_supported(),\n",
        "                          fp16 = not is_bfloat16_supported(),\n",
        "                          per_device_train_batch_size = 6,\n",
        "                          gradient_accumulation_steps = 1,\n",
        "                          num_generations = 6,\n",
        "                          max_prompt_length = 256,\n",
        "                          max_completion_length = 200,\n",
        "                          max_steps = 250,\n",
        "                          save_steps = 250,\n",
        "                          max_grad_norm = 1,\n",
        "                          report_to = 'none',\n",
        "                          output_dir = 'outputs'\n",
        "                          )\n",
        "model.save_lora(\"model_grpo_lora\")"
      ],
      "metadata": {
        "id": "69vH8YWYChbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### Set up the GRPO Trainer"
      ],
      "metadata": {
        "id": "beB7NRjEFwcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = get_gsm8k_question()\n",
        "grpo_trainer = GRPOTrainer(\n",
        "                          model = model,\n",
        "                          processing_class = tokenizer,\n",
        "                          reward_funcs =  [\n",
        "                                            correctness_reward_f,\n",
        "                                            int_reward_func,\n",
        "                                            strict_format_reward_func,\n",
        "                                            soft_format_reward_func,\n",
        "                                            xmlcount_reward_func,\n",
        "                                          ],\n",
        "                          args = training_args,\n",
        "                          train_dataset = dataset\n",
        "                          )\n",
        "\n",
        "grpo_trainer.train()"
      ],
      "metadata": {
        "id": "uz_MMcSwFylD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference with no GRPO training"
      ],
      "metadata": {
        "id": "BdLE1FSsIp5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "                                    [{'role': 'system', 'content': \"calculate pi.\"}],\n",
        "                                    tokenize = False,\n",
        "                                    add_generation_prompt = True\n",
        "                                    )\n",
        "sampling_params = SamplingParams(\n",
        "                                  temperature = 0.2,\n",
        "                                  top_p = 0.95,\n",
        "                                  max_tokes =1024\n",
        "                                )\n",
        "\n",
        "output = model.fast_generate(\n",
        "                            [text],\n",
        "                            sampling_params = sampling_params,\n",
        "                            lora_request = None,\n",
        "                            )[0].outputs[0].text\n",
        "\n",
        "output"
      ],
      "metadata": {
        "id": "mcq8IxlWIwBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference with grpo trained model\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "KlRdAw8gKpKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reasoning_output = model.fast_generate(\n",
        "                            [text],\n",
        "                            sampling_params = sampling_params,\n",
        "                            lora_request = model.load_lora('model_grpo_lora'),\n",
        "                            )[0].outputs[0].text\n",
        "\n",
        "reasoning_output"
      ],
      "metadata": {
        "id": "TRieNS5pKsav"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}