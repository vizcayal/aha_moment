{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOkwd+MH/zQDkcitsPl/apN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vizcayal/aha_moment/blob/main/aha_Llama_countdown_working%20(jun25).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiMU90xp_-tW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm==0.8.5.post1\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel, PatchFastRL\n",
        "from unsloth import is_bfloat16_supported             #check if bfloat16 is supported\n",
        "from datasets import load_dataset, Dataset\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "from vllm import SamplingParams"
      ],
      "metadata": {
        "id": "1Cua9Ww2AQs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#PatchFastRL(\"GRPO\", FastLanguageModel)                # apply patch for training LLMs with Group Relative Policy Optimization (GRPO) used in deepseek\n",
        "max_seq_length = 1024                                  # max sequencen length\n",
        "lora_rank = 32                                        # dim for lora matrix\n",
        "\n",
        "#load the Llama 3.1-8B model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "                                                    #model_name = \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
        "                                                    model_name = \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "                                                    max_seq_length = max_seq_length,\n",
        "                                                    load_in_4bit = True,                                    # False for LoRA 16bit\n",
        "                                                    fast_inference = True,                                  # Enable vLLM fast inference\n",
        "                                                    max_lora_rank = lora_rank,\n",
        "                                                    gpu_memory_utilization = 0.6,\n",
        "                                                    )\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(                                                                   #Parameter efficient Fine-Tuning (peft)\n",
        "                                        model,\n",
        "                                        r = lora_rank,\n",
        "                                        target_modules = [\n",
        "                                                          \"q_proj\",\n",
        "                                                          \"k_proj\",\n",
        "                                                          \"v_proj\",\n",
        "                                                          \"o_proj\",\n",
        "                                                          \"gate_proj\",\n",
        "                                                          \"up_proj\",\n",
        "                                                          \"down_proj\",\n",
        "                                                        ],\n",
        "                                        lora_alpha = lora_rank,\n",
        "                                        use_gradient_checkpointing = \"unsloth\",\n",
        "                                        random_state = 3407,\n",
        "                                        )"
      ],
      "metadata": {
        "id": "u6OhBoYCAUhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_r1_prompt(numbers, target):\n",
        "    r1_prefix = [{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a math assistant, specifically creating arithmetic equations. you first thinks about the reasoning process and then provides the user with the answer.\"\n",
        "      },\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Using the numbers {numbers}, create an equation that equals {target}. You can use only the following basic arithmetic operations (+, -, *, /) one or multiple times but each number can only be used once.\\\n",
        "         Show your work in <think> </think> tags. And return the final equation with all and only ({numbers}) in <answer> </answer> tags, for example if the numbers are 1,2, 3 <answer> (1 + 2) / 3 </answer>. Think step by step inside <think></think> tags.\"\n",
        "      },\n",
        "      {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"Let me solve this step by step.\\n<think>\"\n",
        "      }]\n",
        "    return {\"prompt\": tokenizer.apply_chat_template(r1_prefix, tokenize=False, continue_final_message=True), \"target\": target, \"nums\": numbers}\n",
        "\n",
        "def format_reward_func(completions, target, **kwargs):\n",
        "    \"\"\"\n",
        "    Format: <think>...</think><answer>...</answer>\n",
        "    Args:\n",
        "        completions (list[str]): Generated outputs\n",
        "        target (list[str]): Expected answers\n",
        "\n",
        "      Returns:\n",
        "          list[float]: Reward scores\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "\n",
        "    for completion, gt in zip(completions, target):\n",
        "        try:\n",
        "            # Add synthetic <think> if needed (based on your prompt structure)\n",
        "            completion_with_think = \"<think>\" + completion\n",
        "\n",
        "            # Step 1: Check for <think> and </think> tags\n",
        "            think_match = re.search(r\"<think>(.*?)</think>\", completion_with_think, re.DOTALL)\n",
        "\n",
        "            if think_match is not None:\n",
        "                rewards.append(0.0)\n",
        "                continue\n",
        "\n",
        "            # Step 2: Check for <answer> and </answer> tags\n",
        "            answer_match = re.search(r\"<answer>(.*?)</answer>\", completion_with_think, re.DOTALL)\n",
        "\n",
        "            if answer_match is None:\n",
        "                rewards.append(0.0)\n",
        "                continue\n",
        "\n",
        "            # If both tags are present, reward is 1.0\n",
        "            rewards.append(1.0)\n",
        "\n",
        "        except Exception:\n",
        "            # If any error occurs during the process, reward is 0\n",
        "            rewards.append(0.0)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# def format_reward_func(completions, target, **kwargs):\n",
        "#     \"\"\"\n",
        "#     Format: <think>...</think><answer>...</answer>\n",
        "#     Args:\n",
        "#         completions (list[str]): Generated outputs\n",
        "#         target (list[str]): Expected answers\n",
        "\n",
        "#       Returns:\n",
        "#           list[float]: Reward scores\n",
        "#     \"\"\"\n",
        "#     rewards = []\n",
        "\n",
        "#     # print(\"completions: \", completions)\n",
        "#     # print(\"target: \", target)\n",
        "#     i = 0\n",
        "\n",
        "#     for completion, gt in zip(completions, target):\n",
        "\n",
        "\n",
        "#       try:\n",
        "#         completion = \"<think>\" + completion\n",
        "\n",
        "#         # add synthetic <think> as its already part of the prompt and prefilled for the assistant to more easily match the regex\n",
        "#         #completion = \"<think>\" + completion\n",
        "\n",
        "#         # Check if the format is correct\n",
        "#         regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n",
        "\n",
        "#         match = re.search(regex, completion, re.DOTALL)\n",
        "#         # if the format is not correct, reward is 0\n",
        "#         if match is None or len(match.groups()) != 2:\n",
        "#             rewards.append(0.0)\n",
        "#         else:\n",
        "#             rewards.append(1.0)\n",
        "#       except Exception:\n",
        "#         rewards.append(0.0)\n",
        "#     return rewards\n",
        "\n",
        "def equation_reward_func(completions, target, nums, **kwargs):\n",
        "    \"\"\"\n",
        "    Evaluates completions based on:\n",
        "    2. Mathematical correctness of the answer\n",
        "\n",
        "    Args:\n",
        "        completions (list[str]): Generated outputs\n",
        "        target (list[str]): Expected answers\n",
        "        nums (list[str]): Available numbers\n",
        "\n",
        "    Returns:\n",
        "        list[float]: Reward scores\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for completion, gt, numbers in zip(completions, target, nums):\n",
        "      try:\n",
        "        # add synthetic <think> as its already part of the prompt and prefilled for the assistant to more easily match the regex\n",
        "        completion = \"<think>\" + completion\n",
        "        print(f'{completion = }')\n",
        "        print(f'{gt = }')\n",
        "        print(f'{numbers = }')\n",
        "        # Check if the format is correct\n",
        "        match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n",
        "        if match is None:\n",
        "            rewards.append(0.0)\n",
        "            continue\n",
        "        # Extract the \"answer\" part from the completion\n",
        "        equation = match.group(1).strip()\n",
        "        # Extract all numbers from the equation\n",
        "        used_numbers = [int(n) for n in re.findall(r'\\d+', equation)]\n",
        "        print(f'{equation = }')\n",
        "        print(f'{used_numbers = }')\n",
        "\n",
        "        # Check if all numbers are used exactly once\n",
        "        if sorted(used_numbers) != sorted(numbers):\n",
        "            rewards.append(0.0)\n",
        "            continue\n",
        "        # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace\n",
        "        allowed_pattern = r'^[\\d+\\-*/().\\s]+$'\n",
        "        if not re.match(allowed_pattern, equation):\n",
        "           rewards.append(0.0)\n",
        "           continue\n",
        "\n",
        "        # Evaluate the equation with restricted globals and locals\n",
        "        result = eval(equation, {\"__builtins__\": None}, {})\n",
        "        print(f'{result = }')\n",
        "        # Check if the equation is correct and matches the ground truth\n",
        "        if abs(float(result) - float(gt)) < 1e-5:\n",
        "            rewards.append(1.0)\n",
        "            print('******************we got it****************')\n",
        "\n",
        "        else:\n",
        "            rewards.append(0.0)\n",
        "      except Exception:\n",
        "            # If evaluation fails, reward is 0\n",
        "            rewards.append(0.0)\n",
        "    return rewards\n",
        "\n"
      ],
      "metadata": {
        "id": "ZcjR52ESHNWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "set up params for grpo trainer"
      ],
      "metadata": {
        "id": "NkxYt4qbUN2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert our dataset to the r1 prompt\n",
        "dataset = load_dataset(\"Jiayi-Pan/Countdown-Tasks-3to4\", split = \"train\")\n",
        "dataset = dataset.map(lambda x: generate_r1_prompt(x[\"nums\"], x[\"target\"]))\n",
        "train_test_split = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "test_dataset = train_test_split[\"test\"]"
      ],
      "metadata": {
        "id": "VmMC_1XflNC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = GRPOConfig(\n",
        "                          use_vllm = True,\n",
        "                          learning_rate = 5e-7,\n",
        "                          adam_beta1 = 0.9,\n",
        "                          adam_beta2 = 0.99,\n",
        "                          weight_decay = 0.1,\n",
        "                          warmup_ratio = 0.03,\n",
        "                          lr_scheduler_type = 'cosine',\n",
        "                          optim = 'paged_adamw_8bit',\n",
        "                          logging_steps = 1,\n",
        "                          bf16 = is_bfloat16_supported(),\n",
        "                          fp16 = not is_bfloat16_supported(),\n",
        "                          per_device_train_batch_size = 4,\n",
        "                          gradient_accumulation_steps = 1,\n",
        "                          num_generations = 4,\n",
        "                          max_prompt_length = 512,\n",
        "                          max_completion_length = 2048,\n",
        "                          max_steps = 2000,\n",
        "                          save_steps = 250,\n",
        "                          max_grad_norm = 1,\n",
        "                          report_to = 'none',\n",
        "                          output_dir = 'outputs'\n",
        "                          )\n",
        "\n",
        "\n",
        "seed: 42\n",
        "model.save_lora(\"model_grpo_lora\")"
      ],
      "metadata": {
        "id": "IsrZwzMSUL8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grpo_trainer = GRPOTrainer(\n",
        "                          model = model,\n",
        "                          processing_class = tokenizer,\n",
        "                          reward_funcs=[format_reward_func, equation_reward_func],\n",
        "                          args = training_args,\n",
        "                          train_dataset = dataset\n",
        "                          )\n",
        "\n",
        "grpo_trainer.train()"
      ],
      "metadata": {
        "id": "P9UKquvqXi9P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}